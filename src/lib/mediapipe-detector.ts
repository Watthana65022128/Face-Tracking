// MediaPipe face detection and tracking utilities
import { FaceLandmarker, FilesetResolver, NormalizedLandmark } from '@mediapipe/tasks-vision';

export interface FaceTrackingData {
  isDetected: boolean;
  orientation: {
    yaw: number;
    pitch: number;
    roll: number;
    isLookingAway: boolean;
  };
  confidence: number;
  timestamp: number;
  landmarks?: NormalizedLandmark[];
}

export class MediaPipeDetector {
  private faceLandmarker: FaceLandmarker | null = null;
  private isInitialized: boolean = false;
  private lastDetection: FaceTrackingData | null = null;
  private detectionHistory: FaceTrackingData[] = [];
  private readonly maxHistorySize = 10;

  async initialize(): Promise<boolean> {
    try {
      console.log('üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏´‡∏•‡∏î MediaPipe FaceLandmarker...');
      
      // ‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏´‡∏≤‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );
      console.log('‚úÖ FilesetResolver ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à');

      // ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î model ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ GPU)
      this.faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
          delegate: "CPU"
        },
        outputFaceBlendshapes: false, // ‡∏õ‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞
        outputFacialTransformationMatrixes: false, // ‡∏õ‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞
        runningMode: "VIDEO",
        numFaces: 1
      });

      this.isInitialized = true;
      console.log('‚úÖ MediaPipe FaceLandmarker ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô');
      return true;
    } catch (error) {
      console.error('‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î MediaPipe:', error);
      console.error('‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î:', error instanceof Error ? error.message : String(error));
      
      // ‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
      return await this.initializeFallback();
    }
  }

  private async initializeFallback(): Promise<boolean> {
    try {
      console.log('üîÑ ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î MediaPipe ‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á...');
      
      // ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ CDN ‡∏ï‡πà‡∏≤‡∏á‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.22/wasm"
      );

      this.faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          delegate: "CPU"
        },
        runningMode: "VIDEO",
        numFaces: 1
      });

      this.isInitialized = true;
      console.log('‚úÖ MediaPipe FaceLandmarker ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á');
      return true;
    } catch (fallbackError) {
      console.error('‚ùå ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Å‡πá‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:', fallbackError);
      return false;
    }
  }

  async detectFromVideo(video: HTMLVideoElement, timestamp: number): Promise<FaceTrackingData | null> {
    if (!this.isInitialized || !this.faceLandmarker) {
      console.warn('‚ö†Ô∏è MediaPipe ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô');
      return null;
    }

    try {
      // ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö video readiness
      if (!video || video.readyState < 2) {
        console.warn('‚ö†Ô∏è Video ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° readyState:', video?.readyState);
        return null;
      }

      console.log('üîç ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å detectForVideo...', { timestamp, videoWidth: video.videoWidth, videoHeight: video.videoHeight });
      const results = this.faceLandmarker.detectForVideo(video, timestamp);
      console.log('üìä MediaPipe results:', { 
        hasLandmarks: !!results.faceLandmarks, 
        landmarkCount: results.faceLandmarks?.length || 0,
        firstFaceLandmarks: results.faceLandmarks?.[0]?.length || 0
      });
      
      if (!results.faceLandmarks || results.faceLandmarks.length === 0) {
        console.log('‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô MediaPipe results');
        const noFaceData: FaceTrackingData = {
          isDetected: false,
          orientation: { yaw: 0, pitch: 0, roll: 0, isLookingAway: false },
          confidence: 0,
          timestamp
        };
        
        this.updateHistory(noFaceData);
        return noFaceData;
      }

      const landmarks = results.faceLandmarks[0];
      console.log('‚úÖ ‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤! landmarks:', landmarks.length, '‡∏à‡∏∏‡∏î');
      const trackingData = this.analyzeLandmarks(landmarks, timestamp);
      console.log('üìà tracking data:', trackingData);
      
      this.updateHistory(trackingData);
      this.lastDetection = trackingData;
      
      return trackingData;
    } catch (error) {
      console.error('‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤:', error);
      return null;
    }
  }

  private analyzeLandmarks(landmarks: NormalizedLandmark[], timestamp: number): FaceTrackingData {
    // ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡∏´‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (Face Orientation) - Phase 1 ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ orientation
    const orientation = this.calculateFaceOrientation(landmarks);
    
    return {
      isDetected: true,
      orientation,
      confidence: 0.95, // MediaPipe ‡∏°‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ confidence ‡∏™‡∏π‡∏á
      timestamp,
      landmarks // ‡∏™‡πà‡∏á landmarks ‡∏ó‡∏±‡πâ‡∏á 468 ‡∏à‡∏∏‡∏î‡πÑ‡∏õ‡πÉ‡∏´‡πâ component
    };
  }

  private calculateFaceOrientation(landmarks: NormalizedLandmark[]) {
    // ‡πÉ‡∏ä‡πâ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡∏≤‡∏° MediaPipe FaceMesh 468 landmarks
    const noseTip = landmarks[1];        // ‡∏à‡∏°‡∏π‡∏Å‡∏õ‡∏•‡∏≤‡∏¢
    const leftEyeInner = landmarks[133]; // ‡∏°‡∏∏‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏ã‡πâ‡∏≤‡∏¢
    const rightEyeInner = landmarks[362]; // ‡∏°‡∏∏‡∏°‡πÉ‡∏ô‡∏ï‡∏≤‡∏Ç‡∏ß‡∏≤
    const leftEyeOuter = landmarks[33];   // ‡∏°‡∏∏‡∏°‡∏ô‡∏≠‡∏Å‡∏ï‡∏≤‡∏ã‡πâ‡∏≤‡∏¢  
    const rightEyeOuter = landmarks[263]; // ‡∏°‡∏∏‡∏°‡∏ô‡∏≠‡∏Å‡∏ï‡∏≤‡∏Ç‡∏ß‡∏≤
    const chin = landmarks[18];           // ‡∏Ñ‡∏≤‡∏á
    const forehead = landmarks[10];       // ‡∏´‡∏ô‡πâ‡∏≤‡∏ú‡∏≤‡∏Å

    // ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì yaw (‡∏´‡∏±‡∏ô‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤) ‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡∏≤
    const leftEyeWidth = Math.abs(leftEyeOuter.x - leftEyeInner.x);
    const rightEyeWidth = Math.abs(rightEyeOuter.x - rightEyeInner.x);
    
    // ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ô‡∏ã‡πâ‡∏≤‡∏¢: ‡∏ï‡∏≤‡∏Ç‡∏ß‡∏≤‡∏à‡∏∞‡∏î‡∏π‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á, ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ô‡∏Ç‡∏ß‡∏≤: ‡∏ï‡∏≤‡∏ã‡πâ‡∏≤‡∏¢‡∏à‡∏∞‡∏î‡∏π‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á
    const eyeRatio = leftEyeWidth / rightEyeWidth;
    let yaw = (eyeRatio - 1) * 100; // ‡∏Ñ‡∏π‡∏ì 100 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏á‡∏®‡∏≤
    yaw = Math.max(-60, Math.min(60, yaw)); // ‡∏à‡∏≥‡∏Å‡∏±‡∏î range
    
    // ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì pitch (‡∏´‡∏±‡∏ô‡∏ö‡∏ô-‡∏•‡πà‡∏≤‡∏á) ‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏°‡∏π‡∏Å-‡∏Ñ‡∏≤‡∏á
    const noseToForeheadDistance = Math.abs(noseTip.y - forehead.y);
    const noseToChinDistance = Math.abs(chin.y - noseTip.y);
    
    const verticalRatio = noseToForeheadDistance / noseToChinDistance;
    let pitch = (verticalRatio - 0.6) * 200; // ‡∏õ‡∏£‡∏±‡∏ö scale
    pitch = Math.max(-45, Math.min(45, pitch)); // ‡∏à‡∏≥‡∏Å‡∏±‡∏î range
    
    // ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì roll (‡πÄ‡∏≠‡∏µ‡∏¢‡∏á‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏≠‡∏µ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤
    const eyeCenterY = (leftEyeInner.y + rightEyeInner.y) / 2;
    const leftEyeY = (leftEyeInner.y + leftEyeOuter.y) / 2;
    const rightEyeY = (rightEyeInner.y + rightEyeOuter.y) / 2;
    
    const roll = Math.atan2(rightEyeY - leftEyeY, rightEyeInner.x - leftEyeInner.x) * 180 / Math.PI;

    // ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ "‡∏´‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏à‡∏≠" 
    const YAW_THRESHOLD = 20;      // ‡∏≠‡∏á‡∏®‡∏≤
    const PITCH_THRESHOLD = 15;    // ‡∏≠‡∏á‡∏®‡∏≤
    
    // ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏à‡∏≠
    const isLookingAway = Math.abs(yaw) > YAW_THRESHOLD || Math.abs(pitch) > PITCH_THRESHOLD;

    // Debug logging ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    console.log(`üéØ Face Orientation - Yaw: ${yaw.toFixed(1)}¬∞, Pitch: ${pitch.toFixed(1)}¬∞, Roll: ${roll.toFixed(1)}¬∞, Away: ${isLookingAway}`);
    console.log(`üëÄ Eye Ratio: ${eyeRatio.toFixed(3)}, Vertical Ratio: ${verticalRatio.toFixed(3)}`);

    return { yaw, pitch, roll, isLookingAway };
  }

  private calculateVariance(values: number[]): number {
    if (values.length === 0) return 0;
    
    const mean = values.reduce((sum, val) => sum + val, 0) / values.length;
    const squaredDiffs = values.map(val => Math.pow(val - mean, 2));
    
    return squaredDiffs.reduce((sum, diff) => sum + diff, 0) / values.length;
  }

  private updateHistory(data: FaceTrackingData): void {
    this.detectionHistory.push(data);
    
    // ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î history
    if (this.detectionHistory.length > this.maxHistorySize) {
      this.detectionHistory.shift();
    }
  }

  getLastDetection(): FaceTrackingData | null {
    return this.lastDetection;
  }

  getDetectionHistory(): FaceTrackingData[] {
    return [...this.detectionHistory];
  }

  destroy(): void {
    if (this.faceLandmarker) {
      
      this.faceLandmarker = null;
    }
    this.isInitialized = false;
    this.lastDetection = null;
    this.detectionHistory = [];
    console.log('üßπ MediaPipe detector ‡∏ñ‡∏π‡∏Å‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß');
  }
}